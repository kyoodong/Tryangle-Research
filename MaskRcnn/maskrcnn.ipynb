{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.3.0\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.layers import Conv2D, BatchNormalization, Activation, GlobalAveragePooling2D, Dense, MaxPool2D, Softmax\n",
    "from tensorflow.keras.models import Model\n",
    "import os\n",
    "from glob import glob\n",
    "import math\n",
    "import tensorflow.keras.backend as K\n",
    "import xmltodict\n",
    "from PIL import Image, ImageDraw\n",
    "\n",
    "print(tf.__version__)\n",
    "print(tf.config.list_physical_devices('gpu'))\n",
    "\n",
    "\n",
    "ANCHOR_SIZES = [32, 64, 128]\n",
    "ANCHOR_RATIOS = [[1,1], [math.sqrt(2), 1/math.sqrt(2)], [math.sqrt(3), 1/math.sqrt(3)]]\n",
    "SUBSAMPLE_RATIO = 4\n",
    "\n",
    "class ResUnit(Model):\n",
    "    def __init__(self, filter_in, filter_out, kernel_size):\n",
    "        super(ResUnit, self).__init__()\n",
    "\n",
    "        self.sequence = list()\n",
    "\n",
    "        # Resnet 특유의 덧셈 연산을 위해 입력의 차원(depth, channel)을 맞춰주는 연산\n",
    "        self.identity = Conv2D(filter_out, (1, 1), padding='valid')\n",
    "\n",
    "        # Bottleneck(down sampling)\n",
    "        self.sequence.append(Conv2D(filter_in, (1, 1), padding='valid'))\n",
    "\n",
    "        # Conv\n",
    "        self.sequence.append(Conv2D(filter_in, kernel_size, padding='same'))\n",
    "\n",
    "        # Bottleneck(up sampling)\n",
    "        self.sequence.append(Conv2D(filter_out, (1, 1), padding='valid'))\n",
    "\n",
    "        # BN, Activation\n",
    "        self.sequence.append(BatchNormalization())\n",
    "        self.sequence.append(Activation('relu'))\n",
    "\n",
    "    def __call__(self, images, training):\n",
    "        # Downsampling -> Conv -> Upsampling -> BN -> Activation -> Add\n",
    "        h = images\n",
    "        for unit in self.sequence:\n",
    "            if isinstance(unit, BatchNormalization):\n",
    "                h = unit(h, training=training)\n",
    "            else:\n",
    "                h = unit(h)\n",
    "\n",
    "        # Add\n",
    "        return self.identity(images) + h\n",
    "\n",
    "class ResLayer(Model):\n",
    "    def __init__(self, filter_in, filter_out, kernel_size, iter_count):\n",
    "        super(ResLayer, self).__init__()\n",
    "\n",
    "        self.sequence = list()\n",
    "\n",
    "        # ResUnit 을 iter_count 개수만큼 쌓아올림\n",
    "        # https://eremo2002.tistory.com/76 을 참고하여 ResNet의 논문 커널의 수를 맞춤\n",
    "        for i in range(iter_count):\n",
    "            self.sequence.append(ResUnit(filter_in, filter_out, kernel_size))\n",
    "\n",
    "    def __call__(self, images, training):\n",
    "        for layer in self.sequence:\n",
    "            images = layer(images, training)\n",
    "        return images\n",
    "\n",
    "class RPN(Model):\n",
    "    def __init__(self, image_size, kernel_size, anchor_size, anchor_ratio, subsampling_ratio):\n",
    "        super(RPN, self).__init__()\n",
    "\n",
    "        # ResNet으로 돌릴때는 600 이상 1000 이하로 하는것이 좋음\n",
    "        self.image_size = image_size\n",
    "\n",
    "        # RPN(Region Proposal Network) 학습 시 사용할 Sliding window kernel 의 크기\n",
    "        self.kernel_size = kernel_size\n",
    "\n",
    "        # 앵커 박스 크기\n",
    "        # 실제 오브젝트의 크기가 얼마나 될지 모르기때문에 임의의 크기의 앵커 박스를 두는 것\n",
    "        self.anchor_sizes = anchor_size\n",
    "\n",
    "        # 앵커 박스 비율\n",
    "        self.anchor_ratios = anchor_ratio\n",
    "\n",
    "        # 앵커 박스 개수\n",
    "        self.anchor_num = len(anchor_size) * len(anchor_ratio)\n",
    "\n",
    "        # 이미지 축소 계수\n",
    "        self.subsampling_ratio = subsampling_ratio\n",
    "\n",
    "        self.conv = Conv2D(256, (3, 3), (1, 1), padding='same')\n",
    "\n",
    "        # 바운딩 박스 내에 오브젝트가 있는지 없는지 여부\n",
    "        # 18 = 9(앵커박스 개수) * 2(있다, 없다 각 확률)\n",
    "        # HxWx18 좌표 (h, w)의 각 앵커 박스에 오브젝트가 있을 확률\n",
    "        self.obj_conv = Conv2D(2 * self.anchor_num, (1, 1), padding='valid')\n",
    "\n",
    "        # 바운딩 박스를 regression 하는 conv\n",
    "        # 36 = 9(앵커박스 개수) * 4(바운딩박스 x, y, width, height)\n",
    "        self.bb_conv = Conv2D(4 * self.anchor_num, (1, 1), padding='valid')\n",
    "        self.avgpool = GlobalAveragePooling2D()\n",
    "\n",
    "        self.softmax = Softmax(name='obj_output')\n",
    "        # self.softmax =\n",
    "        self.bbox_fc = Dense(4)\n",
    "\n",
    "    def __call__(self, images):\n",
    "        images = self.conv(images)\n",
    "        obj = self.obj_conv(images)\n",
    "        bb = self.bb_conv(images)\n",
    "        # obj = self.avgpool(obj)\n",
    "        shape = obj.shape\n",
    "        obj = tf.reshape(obj, (-1, shape[1], shape[2], self.anchor_num, 2))\n",
    "        shape = bb.shape\n",
    "        bb = tf.reshape(bb, (-1, shape[1], shape[2], self.anchor_num, 4))\n",
    "        # bb = self.avgpool(bb)\n",
    "        obj = self.softmax(obj)\n",
    "        bb = self.bbox_fc(bb)\n",
    "        bb = tf.map_fn(self.generate_anchors, bb, name='bb_output')\n",
    "\n",
    "\n",
    "        # result = tf.concat([obj, bb], -1)\n",
    "        return [obj, bb]\n",
    "\n",
    "    '''\n",
    "    이 단계에서 9개의 anchor box를 이용하여 classification과 bbox regression을 먼저 구한다. (For 학습)\n",
    "    먼저, CNN에서 뽑아낸 feature map에 대해 3x3 conv filter 256개를 연산하여 depth를 256으로 만든다.\n",
    "    그 후 1x1 conv 두개를 이용하여 각각 classification과 bbox regression을 계산한다.\n",
    "    '''\n",
    "    def generate_anchors(self, feature_map):\n",
    "        print('feature_map = {}'.format(feature_map))\n",
    "        anchor_boxes = list()\n",
    "        subsampled_image_width = int(self.image_size[0] / self.subsampling_ratio)\n",
    "        subsampled_image_height = int(self.image_size[1] / self.subsampling_ratio)\n",
    "        anchor_sizes = np.array(self.anchor_sizes).reshape([-1, 1])\n",
    "        anchor_ratio = np.array(self.anchor_ratios).reshape([1, -1])\n",
    "        anchor_samples = np.matmul(anchor_sizes, anchor_ratio).reshape([-1, 2])\n",
    "\n",
    "        for x in range(subsampled_image_width):\n",
    "            l1 = list()\n",
    "            for y in range(subsampled_image_height):\n",
    "                l2 = list()\n",
    "                for anchor_sample in anchor_samples:\n",
    "                    width, height = anchor_sample[0], anchor_sample[1]\n",
    "                    l2.append((x * subsampled_image_width, y * subsampled_image_height, width, height))\n",
    "                l1.append(l2)\n",
    "            anchor_boxes.append(l1)\n",
    "        anchor_boxes = tf.convert_to_tensor(anchor_boxes)\n",
    "        print('anchor_boxes = {}'.format(anchor_boxes))\n",
    "\n",
    "        x = tf.maximum((feature_map[:,:,:,0] - anchor_boxes[:,:,:,0]) / anchor_boxes[:,:,:,2], 0)\n",
    "        y = tf.maximum((feature_map[:,:,:,1] - anchor_boxes[:,:,:,1]) / anchor_boxes[:,:,:,3], 0)\n",
    "        width = tf.maximum(tf.math.log(feature_map[:,:,:,2] / anchor_boxes[:,:,:,2] + 1e-4), 0)\n",
    "        height = tf.maximum(tf.math.log(feature_map[:,:,:,3] / anchor_boxes[:,:,:,3] + 1e-4), 0)\n",
    "        new_feature_map = tf.stack([x, y, width, height], -1)\n",
    "        # return new_feature_map\n",
    "        return new_feature_map\n",
    "\n",
    "class ResNet(Model):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super(ResNet, self).__init__(args, kwargs)\n",
    "\n",
    "        # ResNet 모델\n",
    "        self.conv1 = Conv2D(64, (7, 7), (2, 2), padding='same')\n",
    "        self.maxpool1 = MaxPool2D((3, 3), (2, 2), padding='same')\n",
    "        self.res1 = ResLayer(64, 256, (3, 3), 3)\n",
    "        self.res2 = ResLayer(128, 512, (3, 3), 4)\n",
    "        # self.res3 = ResLayer(256, 1024, (3, 3), 6)\n",
    "        # self.res4 = ResLayer(512, 2048, (3, 3), 3)  # channel = 2048\n",
    "        # self.maxpool2 = MaxPool2D((4, 4), (4, 4))  # channel = 128\n",
    "\n",
    "    def __call__(self, images, training):\n",
    "        print(images.shape)\n",
    "        images = self.conv1(images)\n",
    "        print(images.shape)\n",
    "        images = self.maxpool1(images)\n",
    "        print(images.shape)\n",
    "        images = self.res1(images, training)\n",
    "        print(images.shape)\n",
    "        images = self.res2(images, training)\n",
    "        print(images.shape)\n",
    "        # images = self.res3(images, training)\n",
    "        # images = self.res4(images, training)\n",
    "        # images = self.maxpool2(images)\n",
    "        # images = self.rpn(images)\n",
    "        return images"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "def cal_iou(anchor, ground_truth):\n",
    "    anchor_area = anchor[:,:,:,2] * anchor[:,:,:,3]\n",
    "    ground_truth_area = ground_truth[:,2] * ground_truth[:,3]\n",
    "    left = tf.maximum(anchor[:,:,:,0], ground_truth[:,0])\n",
    "    top = tf.maximum(anchor[:,:,:,1], ground_truth[:,1])\n",
    "    right = tf.minimum(anchor[:,:,:,0] + anchor[:,:,:,2], ground_truth[:,0] + ground_truth[:,2])\n",
    "    bottom = tf.minimum(anchor[:,:,:,1] + anchor[:,:,:,3], ground_truth[:,1] + ground_truth[:,3])\n",
    "    width = tf.maximum(right - left, 0)\n",
    "    height = tf.maximum(bottom - top, 0)\n",
    "\n",
    "    intersection_area = width * height\n",
    "    # iou = tf.concat([1 - (intersection_area / (anchor_area + ground_truth_area - intersection_area)),\n",
    "    #                   intersection_area / (anchor_area + ground_truth_area - intersection_area)], -1)\n",
    "    # iou = tf.reshape(iou, [-1, iou.shape[1], iou.shape[2], anchor.shape[3], 2])\n",
    "    # return iou\n",
    "    return intersection_area / (anchor_area + ground_truth_area - intersection_area)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 / 51\n",
      "0 / 49\n"
     ]
    }
   ],
   "source": [
    "# dataset 준비\n",
    "train_dir = 'train/VOCdevkit/VOC2007'\n",
    "test_dir = 'test/VOCdevkit/VOC2007'\n",
    "\n",
    "\n",
    "def get_dataset(dir, image_size):\n",
    "    annotation_files = glob(\"{}/Annotations/0011*\".format(dir))\n",
    "    image_files = glob(\"{}/JPEGImages/0011*\".format(dir))\n",
    "\n",
    "    images, labels = list(), list()\n",
    "\n",
    "    count = 0\n",
    "    for annotation_file in annotation_files:\n",
    "        if count % 100 == 0:\n",
    "            print(\"{} / {}\".format(count, len(annotation_files)))\n",
    "        count += 1\n",
    "        file = open(annotation_file, mode='r')\n",
    "        file_data = file.read()\n",
    "        annotation = xmltodict.parse(file_data)['annotation']\n",
    "        filename = annotation['filename']\n",
    "        width = float(annotation['size']['width'])\n",
    "        height = float(annotation['size']['height'])\n",
    "        channel = float(annotation['size']['depth'])\n",
    "        im = Image.open(\"{}/JPEGImages/{}\".format(dir, filename)).resize(image_size)\n",
    "        image = np.array(im)\n",
    "        image = (image / 255.0).astype(np.float32)\n",
    "\n",
    "        def append(obj):\n",
    "            name = obj['name']\n",
    "            bndbox = obj['bndbox']\n",
    "            x = int(bndbox['xmin'])\n",
    "            y = int(bndbox['ymin'])\n",
    "            bnd_width = int(bndbox['xmax']) - x\n",
    "            bnd_height = int(bndbox['ymax']) - y\n",
    "\n",
    "            # 224x224 사이즈로 정규화\n",
    "            x = np.float32((float(x) / width) * image_size[0])\n",
    "            y = np.float32((float(y) / height) * image_size[1])\n",
    "            bnd_width = np.float32((float(bnd_width) / width) * image_size[0])\n",
    "            bnd_height = np.float32((float(bnd_height) / height) * image_size[1])\n",
    "            images.append(image)\n",
    "            l = list()\n",
    "            for _ in range(256):\n",
    "                l.append(np.array([0.0, 1.0, x, y, bnd_width, bnd_height]))\n",
    "            labels.append(l)\n",
    "\n",
    "        object = annotation['object']\n",
    "        if isinstance(object, list):\n",
    "            for obj in object:\n",
    "                append(obj)\n",
    "        else:\n",
    "            obj = object\n",
    "            append(obj)\n",
    "\n",
    "    return np.array(images), np.array(labels)\n",
    "\n",
    "\n",
    "IMAGE_SIZE = (224, 224)\n",
    "train_x, train_y = get_dataset(train_dir, IMAGE_SIZE)\n",
    "test_x, test_y = get_dataset(test_dir, IMAGE_SIZE)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "BATCH_SIZE = 16\n",
    "train_ds = tf.data.Dataset.from_tensor_slices((train_x, train_y)).shuffle(1000).batch(BATCH_SIZE)\n",
    "test_ds = tf.data.Dataset.from_tensor_slices((test_x, test_y)).batch(BATCH_SIZE)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "def loss_function(y_true, y_pred):\n",
    "    print('y_pred = {}'.format(y_pred))\n",
    "    print('y_true = {}'.format(y_true))\n",
    "\n",
    "    classification_pred, box_regression_pred = tf.split(y_pred, [2, 4], -1)\n",
    "    classification_true, box_regression_true = tf.split(y_true, [2, 4], -1)\n",
    "\n",
    "    def get_iou(inputs):\n",
    "        anchor = inputs[0]\n",
    "        ground_truth = inputs[1]\n",
    "        anchor_area = anchor[:,:,:,2] * anchor[:,:,:,3]\n",
    "\n",
    "        def gt_map(gt):\n",
    "            print('gt = {}'.format(gt))\n",
    "            return gt\n",
    "\n",
    "        new_gt = tf.map_fn(gt_map, ground_truth)\n",
    "        ground_truth_area = ground_truth[:,2] * ground_truth[:,3]\n",
    "        left = tf.maximum(anchor[:,:,:,0], ground_truth[:,0])\n",
    "        top = tf.maximum(anchor[:,:,:,1], ground_truth[:,1])\n",
    "        right = tf.minimum(anchor[:,:,:,0] + anchor[:,:,:,2], ground_truth[:,0] + ground_truth[:,2])\n",
    "        bottom = tf.minimum(anchor[:,:,:,1] + anchor[:,:,:,3], ground_truth[:,1] + ground_truth[:,3])\n",
    "        width = tf.maximum(right - left, 0)\n",
    "        height = tf.maximum(bottom - top, 0)\n",
    "\n",
    "        intersection_area = width * height\n",
    "        return [1 - (intersection_area / (anchor_area + ground_truth_area - intersection_area)),\n",
    "                          intersection_area / (anchor_area + ground_truth_area - intersection_area)]\n",
    "\n",
    "    iou, invert_iou = tf.map_fn(get_iou, [box_regression_pred, box_regression_true])\n",
    "\n",
    "    def generate_anchor_box(inputs):\n",
    "        anchor_boxes = list()\n",
    "        subsampled_image_width = int(IMAGE_SIZE[0] / SUBSAMPLE_RATIO)\n",
    "        subsampled_image_height = int(IMAGE_SIZE[1] / SUBSAMPLE_RATIO)\n",
    "        anchor_sizes = np.array(ANCHOR_SIZES).reshape([-1, 1])\n",
    "        anchor_ratio = np.array(ANCHOR_RATIOS).reshape([1, -1])\n",
    "        anchor_samples = np.matmul(anchor_sizes, anchor_ratio).reshape([-1, 2])\n",
    "\n",
    "        for y in range(subsampled_image_width):\n",
    "            l1 = list()\n",
    "            for x in range(subsampled_image_height):\n",
    "                l2 = list()\n",
    "                for anchor_sample in anchor_samples:\n",
    "                    width, height = anchor_sample[0], anchor_sample[1]\n",
    "                    l2.append((x * SUBSAMPLE_RATIO, y * SUBSAMPLE_RATIO, width, height))\n",
    "                l1.append(l2)\n",
    "            anchor_boxes.append(l1)\n",
    "        anchor_boxes = np.array(anchor_boxes)\n",
    "\n",
    "        print('inputs = {}'.format(inputs))\n",
    "\n",
    "        x = tf.maximum((inputs[:,:,:,0] - anchor_boxes[:,:,:,0]) / anchor_boxes[:,:,:,2], 0)\n",
    "        y = tf.maximum((inputs[:,:,:,1] - anchor_boxes[:,:,:,1]) / anchor_boxes[:,:,:,3], 0)\n",
    "        width = tf.maximum(tf.math.log(inputs[:,:,:,2] / anchor_boxes[:,:,:,2] + 1e-4), 0)\n",
    "        height = tf.maximum(tf.math.log(inputs[:,:,:,3] / anchor_boxes[:,:,:,3] + 1e-4), 0)\n",
    "        new_feature_map = tf.stack([x, y, width, height], -1)\n",
    "        return new_feature_map\n",
    "\n",
    "    box_regression_true = tf.map_fn(generate_anchor_box, box_regression_true)\n",
    "    box_regression_pred = tf.map_fn(generate_anchor_box, box_regression_pred)\n",
    "\n",
    "    def map(x):\n",
    "        x = tf.reshape(x, [-1, 6])\n",
    "        x = tf.sort(x, direction='DESCENDING', axis=-1)\n",
    "        x = tf.slice(x, [0, 0], [256, -1])\n",
    "        return x\n",
    "\n",
    "    # box_loss = smooth L1 loss\n",
    "    def cal_box_loss(x, y):\n",
    "        gamma = 10\n",
    "        HUBER_DELTA = 1\n",
    "        box_loss = K.abs(x - y)\n",
    "        box_loss = K.switch(box_loss < HUBER_DELTA, 0.5 * (box_loss ** 2), box_loss - 0.5)\n",
    "        box_loss = K.sum(box_loss, axis=-1)\n",
    "        return gamma * box_loss\n",
    "\n",
    "    def get_iou(anchor):\n",
    "        ground_truth = anchor[1]\n",
    "        anchor = anchor[0]\n",
    "        anchor_area = anchor[:,2] * anchor[:,3]\n",
    "        ground_truth_area = ground_truth[:,2] * ground_truth[:,3]\n",
    "        left = tf.maximum(anchor[:,0], ground_truth[:,0])\n",
    "        top = tf.maximum(anchor[:,1], ground_truth[:,1])\n",
    "        right = tf.minimum(anchor[:,0] + anchor[:,2], ground_truth[:,0] + ground_truth[:,2])\n",
    "        bottom = tf.minimum(anchor[:,1] + anchor[:,3], ground_truth[:,1] + ground_truth[:,3])\n",
    "        width = tf.maximum(right - left, 0)\n",
    "        height = tf.maximum(bottom - top, 0)\n",
    "\n",
    "        intersection_area = width * height\n",
    "        return [1 - (intersection_area / (anchor_area + ground_truth_area - intersection_area)),\n",
    "                          intersection_area / (anchor_area + ground_truth_area - intersection_area)]\n",
    "\n",
    "    iou, invert_iou = tf.map_fn(get_iou, [box_regression_pred, box_regression_true])\n",
    "    iou = tf.concat([iou, invert_iou], -1)\n",
    "    iou = tf.reshape(iou, [-1, invert_iou.shape[1], 2])\n",
    "    box_loss = cal_box_loss(box_regression_pred, box_regression_true)\n",
    "    cross_entropy = K.categorical_crossentropy(classification_pred, iou)\n",
    "    result = cross_entropy + box_loss\n",
    "    return result"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'str' object has no attribute 'op'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mAttributeError\u001B[0m                            Traceback (most recent call last)",
      "\u001B[0;32m<ipython-input-6-ac912bb2e081>\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[0;32m----> 1\u001B[0;31m \u001B[0mmodel\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mResNet\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0moutputs\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;34m'bb_output'\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m'obj_output'\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m      2\u001B[0m \u001B[0mmodel\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mcompile\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0moptimizer\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;34m'adam'\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mloss\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;34m{\u001B[0m\u001B[0;34m'bb_output'\u001B[0m\u001B[0;34m:\u001B[0m \u001B[0mloss_function\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m'obj_output'\u001B[0m\u001B[0;34m:\u001B[0m \u001B[0mloss_function\u001B[0m\u001B[0;34m}\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mmetrics\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;34m'accuracy'\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      3\u001B[0m \u001B[0mmodel\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mfit\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mtrain_ds\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mbatch_size\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mBATCH_SIZE\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      4\u001B[0m \u001B[0mpredictions\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mmodel\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mpredict\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mtest_ds\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mbatch_size\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mBATCH_SIZE\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      5\u001B[0m \u001B[0mprint\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m'predictions = {}'\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mformat\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mpredictions\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m<ipython-input-1-b0e1766d4786>\u001B[0m in \u001B[0;36m__init__\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m    162\u001B[0m \u001B[0;32mclass\u001B[0m \u001B[0mResNet\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mModel\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    163\u001B[0m     \u001B[0;32mdef\u001B[0m \u001B[0m__init__\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m*\u001B[0m\u001B[0margs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 164\u001B[0;31m         \u001B[0msuper\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mResNet\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m__init__\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0margs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    165\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    166\u001B[0m         \u001B[0;31m# ResNet 모델\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/hdd/gomson/MaskRcnn/venv/lib/python3.7/site-packages/tensorflow/python/training/tracking/base.py\u001B[0m in \u001B[0;36m_method_wrapper\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m    455\u001B[0m     \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_self_setattr_tracking\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;32mFalse\u001B[0m  \u001B[0;31m# pylint: disable=protected-access\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    456\u001B[0m     \u001B[0;32mtry\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 457\u001B[0;31m       \u001B[0mresult\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mmethod\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m*\u001B[0m\u001B[0margs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    458\u001B[0m     \u001B[0;32mfinally\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    459\u001B[0m       \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_self_setattr_tracking\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mprevious_value\u001B[0m  \u001B[0;31m# pylint: disable=protected-access\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/hdd/gomson/MaskRcnn/venv/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py\u001B[0m in \u001B[0;36m__init__\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m    253\u001B[0m         not isinstance(self, functional.Functional)):\n\u001B[1;32m    254\u001B[0m       \u001B[0minject_functional_model_class\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m__class__\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 255\u001B[0;31m       \u001B[0mfunctional\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mFunctional\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m__init__\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m*\u001B[0m\u001B[0margs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    256\u001B[0m       \u001B[0;32mreturn\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    257\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/hdd/gomson/MaskRcnn/venv/lib/python3.7/site-packages/tensorflow/python/training/tracking/base.py\u001B[0m in \u001B[0;36m_method_wrapper\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m    455\u001B[0m     \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_self_setattr_tracking\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;32mFalse\u001B[0m  \u001B[0;31m# pylint: disable=protected-access\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    456\u001B[0m     \u001B[0;32mtry\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 457\u001B[0;31m       \u001B[0mresult\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mmethod\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m*\u001B[0m\u001B[0margs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    458\u001B[0m     \u001B[0;32mfinally\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    459\u001B[0m       \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_self_setattr_tracking\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mprevious_value\u001B[0m  \u001B[0;31m# pylint: disable=protected-access\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/hdd/gomson/MaskRcnn/venv/lib/python3.7/site-packages/tensorflow/python/keras/engine/functional.py\u001B[0m in \u001B[0;36m__init__\u001B[0;34m(self, inputs, outputs, name, trainable)\u001B[0m\n\u001B[1;32m    113\u001B[0m     \u001B[0;31m#     'arguments during initialization. Got an unexpected argument:')\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    114\u001B[0m     \u001B[0msuper\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mFunctional\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m__init__\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mname\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mname\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mtrainable\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mtrainable\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 115\u001B[0;31m     \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_init_graph_network\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0minputs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0moutputs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    116\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    117\u001B[0m   \u001B[0;34m@\u001B[0m\u001B[0mtrackable\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mno_automatic_dependency_tracking\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/hdd/gomson/MaskRcnn/venv/lib/python3.7/site-packages/tensorflow/python/training/tracking/base.py\u001B[0m in \u001B[0;36m_method_wrapper\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m    455\u001B[0m     \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_self_setattr_tracking\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;32mFalse\u001B[0m  \u001B[0;31m# pylint: disable=protected-access\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    456\u001B[0m     \u001B[0;32mtry\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 457\u001B[0;31m       \u001B[0mresult\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mmethod\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m*\u001B[0m\u001B[0margs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    458\u001B[0m     \u001B[0;32mfinally\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    459\u001B[0m       \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_self_setattr_tracking\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mprevious_value\u001B[0m  \u001B[0;31m# pylint: disable=protected-access\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/hdd/gomson/MaskRcnn/venv/lib/python3.7/site-packages/tensorflow/python/keras/engine/functional.py\u001B[0m in \u001B[0;36m_init_graph_network\u001B[0;34m(self, inputs, outputs)\u001B[0m\n\u001B[1;32m    140\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    141\u001B[0m     \u001B[0;32mif\u001B[0m \u001B[0many\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;32mnot\u001B[0m \u001B[0mhasattr\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mtensor\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m'_keras_history'\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;32mfor\u001B[0m \u001B[0mtensor\u001B[0m \u001B[0;32min\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0moutputs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 142\u001B[0;31m       \u001B[0mbase_layer_utils\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mcreate_keras_history\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_nested_outputs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    143\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    144\u001B[0m     \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_validate_graph_inputs_and_outputs\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/hdd/gomson/MaskRcnn/venv/lib/python3.7/site-packages/tensorflow/python/keras/engine/base_layer_utils.py\u001B[0m in \u001B[0;36mcreate_keras_history\u001B[0;34m(tensors)\u001B[0m\n\u001B[1;32m    189\u001B[0m       \u001B[0mthe\u001B[0m \u001B[0mraw\u001B[0m \u001B[0mTensorflow\u001B[0m \u001B[0moperations\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    190\u001B[0m   \"\"\"\n\u001B[0;32m--> 191\u001B[0;31m   \u001B[0m_\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mcreated_layers\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0m_create_keras_history_helper\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mtensors\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mset\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m[\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    192\u001B[0m   \u001B[0;32mreturn\u001B[0m \u001B[0mcreated_layers\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    193\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/hdd/gomson/MaskRcnn/venv/lib/python3.7/site-packages/tensorflow/python/keras/engine/base_layer_utils.py\u001B[0m in \u001B[0;36m_create_keras_history_helper\u001B[0;34m(tensors, processed_ops, created_layers)\u001B[0m\n\u001B[1;32m    224\u001B[0m                        \u001B[0;34m'op wrapping. Please wrap these ops in a Lambda layer: '\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    225\u001B[0m                        '\\n\\n```\\n{example}\\n```\\n'.format(example=example))\n\u001B[0;32m--> 226\u001B[0;31m     \u001B[0mop\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mtensor\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mop\u001B[0m  \u001B[0;31m# The Op that created this Tensor.\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    227\u001B[0m     \u001B[0;32mif\u001B[0m \u001B[0mop\u001B[0m \u001B[0;32mnot\u001B[0m \u001B[0;32min\u001B[0m \u001B[0mprocessed_ops\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    228\u001B[0m       \u001B[0;31m# Recursively set `_keras_history`.\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mAttributeError\u001B[0m: 'str' object has no attribute 'op'"
     ]
    }
   ],
   "source": [
    "model = tf.keras.models.Sequential()\n",
    "model.add(ResNet())\n",
    "model.add(RPN((224, 224), 3, ANCHOR_SIZES, ANCHOR_RATIOS, SUBSAMPLE_RATIO))\n",
    "model.compile(optimizer='adam', loss={'bb_output': loss_function, 'obj_output': loss_function}, metrics=['accuracy'])\n",
    "model.fit(train_ds, batch_size=BATCH_SIZE)\n",
    "predictions = model.predict(test_ds, batch_size=BATCH_SIZE)\n",
    "print('predictions = {}'.format(predictions))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "a = tf.constant(np.array([[[[9,1,7,10], [3,4,5,6]]], [[[6, 5,4,8], [1, 2, 3, 4]]]]).astype(np.int32))\n",
    "a = tf.constant(np.array([[1], [4]]).astype(np.int32))\n",
    "b = tf.constant(10 - a)\n",
    "print(a, b)\n",
    "c = tf.concat([a, b], -1)\n",
    "print(c)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}